# Scrap Links

This project scrapes all the links of a website by checking the robots.txt file and their sitemaps.

---

## Installation

1. Clone the repository:

```bash

git clone https://github.com/AashishKatila/extract-all-links
cd scrap

```

2. Install dependencies:

```bash

npm install

```

## Usage

1. Scrape a single page:

```bash

npx ts-node src/index.ts <base_url>


```

- Output will be saved to robots-results and sitemaps-url.

---
